#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Nov  1 13:10:32 2021

@author: annabellestanley
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
import seaborn as sns

from sklearn.model_selection import train_test_split # import multiple models
from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV
# import recursive feature elimination
from sklearn.feature_selection import RFE # imports datasets from scikit-learn
from sklearn.datasets import load_boston # evaluation metric: MSE
from sklearn.metrics import mean_squared_error
      # packages for residual analysis
from statsmodels.graphics.gofplots import ProbPlot
# box-cox tranformation
from scipy.stats import boxcox
 # stat-models
import statsmodels.formula.api as smf

#import os
#print os.getcwd()  # Prints the current working directory


import os
path="/Users/annabellestanley/Documents/R_projects/Classes/STAT_6420/STAT_6420_project/HW8"
os.chdir(path)


####### Problem 3

### ### ### 3a 

# load dataset with header and row names
df1 = pd.read_csv("data8.3.csv", header=0, index_col=0)
df1.shape  # (322, 16)
                     # Now remove the missing values
df2 = df1.dropna()

df2.shape  # (263, 16)
# create predictor variables and the reponse
X = df2.drop(columns=['Salary', 'League', 'Division', 'NewLeague'])
Y = df2['Salary']
                     
##data3 = pd.read_csv("data8.3.csv", delim_whitespace=True) #old way to read in data


### ### ### 3b

    ### Here we split the whole dataset into two parts like before, one for training and the other for testing.
# do a one-time split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=31)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape) 



# number of selected features
nof = 0
best_mse = 200000 # initialized at a big value 
mse_list = []

for n in range(16):
    # create a model
    model = LinearRegression()
    rfe = RFE(model, n+1)
    
    # export selected data from RFE
    X_train_rfe = rfe.fit_transform(X_train, y_train)
    X_test_rfe = rfe.transform(X_test)
    
    # fit the new model
    model.fit(X_train_rfe, y_train)
    y_pred = model.predict(X_test_rfe)
    
    # mean_squared_error(y_true, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    mse_list.append(mse)
    
    if(mse < best_mse):
        best_mse = mse
        nof = n+1

print(f"Optimal number of features: {nof}\nMSE: {best_mse:.4f}")

    ### plot
    
# MSE against number of variables selected in model
                    #
plt.figure(figsize=(10,8))
plt.plot(range(1, X.shape[-1]+1), np.log(mse_list), linestyle='-', marker='*', markersize=10, color='blue')
plt.title('Variable selection', fontsize=16)
plt.xlabel('Number of predictor variables', fontsize=16)
plt.ylabel('log(MSE)', fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
                    


### ### ### 3c 

modelLR = LinearRegression()
rfe = RFE(modelLR, 7)
                    # Now you have to bring things back from RFE
                    # export selected data from RFE
X_rfe = rfe.fit_transform(X, Y)
                    # fit the new model
modelLR.fit(X_rfe, Y)
                    # fitted values and residuals of the linear model
fitLR = modelLR.predict(X_rfe)
resLR = Y - fitLR  # residuals: y_true - y_fit
print(X.columns[rfe.support_])
print(np.round(modelLR.coef_,3))
print(np.round(modelLR.intercept_,3))    

# pre-transformation model summary
                    # get the summary table via statsmodels
df3 = df2.copy()
df3.drop(columns=['League', 'Division', 'NewLeague'])
                    # This is same as before
Formula = 'Salary~AtBat+Hits+HmRun+Runs+Walks+Years+CHmRun'
g = smf.ols(formula=Formula, data=df3).fit()
print(g.summary())     

 # Residual analysis plots ##
                # (See some techniques to make the plot look better)
                #
                # Variable selection
                #
plt.rcParams.update({'font.size': 16})
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
df3 = pd.DataFrame({'Residual': resLR, 'Fitted': fitLR})
df3.plot(kind='scatter', x='Fitted', y='Residual', c='blue',ax=ax1)
ax1.set_title('Variable selection')
ax1.hlines(0, 0, 1500, linestyles='dashed', colors='gray')
        
QQ = ProbPlot(resLR)
QQ.qqplot(line='s', color='blue', lw=1, ax=ax2)
plt.title("Normal probability plot")
                # plt.savefig('HW7Q5_Figure_2.png')
plt.show()
                #
                # Box-cox transformation ##
                #
yt, lmbda = boxcox(Y)
print(round(lmbda, 3)) # optimal value
                # Once you decided about the transformation, you can use np to do it.
yt = np.log(Y) # log transformation
                #                 


### ### ### 3d - repeating steps 3b and 3c with new Y variable 



    ### Here we split the whole dataset into two parts like before, one for training and the other for testing.
# do a one-time split
X_train, X_test, y_train, y_test = train_test_split(X, yt, test_size=0.3, random_state=31)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape) 



# number of selected features
nof = 0
best_mse = 200000 # initialized at a big value 
mse_list = []

for n in range(16):
    # create a model
    model = LinearRegression()
    rfe = RFE(model, n+1)
    
    # export selected data from RFE
    X_train_rfe = rfe.fit_transform(X_train, y_train)
    X_test_rfe = rfe.transform(X_test)
    
    # fit the new model
    model.fit(X_train_rfe, y_train)
    y_pred = model.predict(X_test_rfe)
    
    # mean_squared_error(y_true, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    mse_list.append(mse)
    
    if(mse < best_mse):
        best_mse = mse
        nof = n+1

print(f"Optimal number of features: {nof}\nMSE: {best_mse:.4f}")

    ### plot
    
# MSE against number of variables selected in model
                    #
plt.figure(figsize=(10,8))
plt.plot(range(1, X.shape[-1]+1), np.log(mse_list), linestyle='-', marker='*', markersize=10, color='blue')
plt.title('Variable selection', fontsize=16)
plt.xlabel('Number of predictor variables', fontsize=16)
plt.ylabel('log(MSE)', fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)


  # To repeat part c... 
  # decide how many variables you should have in the final model. 
  # Report the final fitted model. Draw the residual versus fitted plot, Q-Q plot and confirm that the final model is reasonable.
  #From previous plot generated, don't need more than 10 variables now


 # Residual analysis plots ##
#!!!!!!!!!!!!!!!!!!!!!!!!!! 
modelLR = LinearRegression()
rfe = RFE(modelLR, 10)
                    # Now you have to bring things back from RFE
                    # export selected data from RFE
X_rfe = rfe.fit_transform(X, yt)
                    # fit the new model
modelLR.fit(X_rfe, yt)
                    # fitted values and residuals of the linear model
fitLR = modelLR.predict(X_rfe)
resLR = yt - fitLR  # residuals: y_true - y_fit


Formula = 'yt ~ X_rfe'
h = smf.ols(formula= Formula, data=df3).fit() 
h.fit(formula= Formula, data=df3)
print(h.summary())


plt.rcParams.update({'font.size': 16})
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
df3 = pd.DataFrame({'Residual': resLR, 'Fitted': fitLR})
df3.plot(kind='scatter', x='Fitted', y='Residual', c='blue',ax=ax1)
ax1.set_title('Variable selection')
ax1.hlines(0, 2.5, 10, linestyles='dashed', colors='gray')
        
QQ = ProbPlot(resLR)
QQ.qqplot(line='s', color='blue', lw=1, ax=ax2)
plt.title("Normal probability plot")
plt.show()





Formula = 'Y ~ X'
g = smf.ols(formula= Formula, data=df2).fit() 
print(g.summary())


                    

###  Write down the variables selected in either cases side by side
     ### Now lets compare the full model with a transformed Y to the Model in 5c with only 7 predictors 

# !!!!!!!!!!!!!!
 
 
 ### What did you learn from this exercise?





### ### ### 3e - Perform LASSO regression on the transformed variable 

## Need to set up with the entire dataset... 

gLasso = LassoCV(cv = 10).fit(X, yt)

print(gLasso.alpha_) ## alpha value
print(gLasso.coef_) #coefficients
print(gLasso.intercept_) # intercept


## Report the final model you get with non-zero regression coefficients.

# Get 8 non zero intercepts - how to I match these/write into a model? 


## Residual analysis

fitLR = gLasso.predict(X)
resLR = yt - fitLR  # residuals: y_true - y_fit


plt.rcParams.update({'font.size': 16})
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
df3 = pd.DataFrame({'Residual': resLR, 'Fitted': fitLR})
df3.plot(kind='scatter', x='Fitted', y='Residual', c='blue',ax=ax1)
ax1.set_title('Variable selection')
ax1.hlines(0, 4.5, 7.5, linestyles='dashed', colors='gray')
        
QQ = ProbPlot(resLR)
QQ.qqplot(line='s', color='blue', lw=1, ax=ax2)
plt.title("Normal probability plot")
plt.show()








### ### ### 3f- Perform Ridge regression on the transformed variable


gRidge= RidgeCV(cv = 10).fit(X, yt)
modelRidge = RidgeCV(cv=10, alphas=np.exp2(np.arange(-3, 15, 0.5))).fit(X, yt)

print(gRidge.alpha_) ## alpha value
print(gRidge.coef_) #coefficients
print(gRidge.intercept_) # intercept

print(modelRidge.alpha_) ## alpha value
print(modelRidge.coef_) #coefficients
print(modelRidge.intercept_) # intercept


print(round(modelRidge.intercept_, 4)) #### code from Shiyuan 


## Report the final model you get with non-zero regression coefficients.

# All non zero intercepts - how to I match these/write into a model? 


## Residual analysis

fitLR = gRidge.predict(X)
resLR = yt - fitLR  # residuals: y_true - y_fit


plt.rcParams.update({'font.size': 16})
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
df3 = pd.DataFrame({'Residual': resLR, 'Fitted': fitLR})
df3.plot(kind='scatter', x='Fitted', y='Residual', c='blue',ax=ax1)
ax1.set_title('Variable selection')
ax1.hlines(0, 4, 9, linestyles='dashed', colors='gray')
        
QQ = ProbPlot(resLR)
QQ.qqplot(line='s', color='blue', lw=1, ax=ax2)
plt.title("Normal probability plot")
plt.show()




### ### ### 3g- Draw a plot where you compare the Ridge and LASSO regression coefficients, 
### along with the coefficients of the final model chosen in part (3d), 
### as well as the full model with transformed response.


gRidge= RidgeCV(cv = 10).fit(X, yt)
gLasso = LassoCV(cv = 10).fit(X, yt)

#Getting error when copy code from assignment that coef_LR is undefined
# I think that this is suppose to correspond to model from 3d but need to clarify what that is 
# For now setting as the same as modelLR0




print(gLasso.coef_) #coefficients


modelLR0 = LinearRegression()
modelLR0.fit(X, yt)


coef_LR = modelLR.coef_


# adj R2 for full model
1-(1-modelLR0.score(X, yt))*(X.shape[0]-1)/(X.shape[0]-X.shape[1]-1)

plt.figure(figsize=(10, 8))
plt.plot(coef_LR, alpha=0.7, linestyle='none', marker='*', markersize=10, color='red', label=r'OLS_4', zorder=7) # zorder for ordering the markers
plt.plot(modelLR0.coef_, alpha=1.0, linestyle='none', marker='x',
markersize=10, color='black', label=r'OLS_all', zorder=7) # zorder for ordering the markers
plt.plot(gLasso.coef_, alpha=0.5, linestyle='none', marker='d',
markersize=10, color='blue', label=r'Lasso') # alpha here is for transparency
plt.plot(gRidge.coef_, alpha=0.4, linestyle='none', marker='o',
markersize=10, color='green', label=r'Ridge')

plt.xlabel('Coefficient Index', fontsize=16)
plt.ylabel('Coefficient Magnitude', fontsize=16)
plt.legend(fontsize=20, loc=1)
# plt.savefig('HW7Q5_Figure_8.png')
plt.show()




   ####### Problem 4

### ### ### 4a 
      
import os
path="/Users/annabellestanley/Documents/R_projects/Classes/STAT_6420/STAT_6420_project/HW8"
os.chdir(path)
               
data4 = pd.read_csv("data8.4.txt", delim_whitespace=True) #old way to read in data

import seaborn as sns

    # preview of first 5 of the explanatory variables
data4.head()
data4.shape
    # pandas drop a column with drop function
X = data4.drop(["RATE"], axis=1)
y = data4["RATE"]
    # Using Pearson Correlation
plt.figure(figsize=(12,10))
cor = X.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()               
                    

### ### ### 4b
    ## Find the condition number and VIFs. Is collinearity an issue here?
    


    
### ### ### 4c
    ## Perform backward elimination.
    
    
# Adding constant column of ones, mandatory for sm.OLS model
X_1 = sm.add_constant(X) # Fitting sm.OLS model
g = sm.OLS(y,X_1).fit()
print(g.summary())
    #Backward Elimination
cols = list(X.columns)
pmax = 1
while (len(cols)>0):
            p = []
            X_1 = X[cols]
            X_1 = sm.add_constant(X_1)
            model = sm.OLS(y,X_1).fit()
            p = model.pvalues.tolist()
            p = p[1:len(p)]
            pmax = max(p)
            feature_with_p_max = np.argmax(p)
            if(pmax>0.05):
                    cols.remove(cols[feature_with_p_max])
            else:
                break

selected_features_BE = cols
print(selected_features_BE)
    
    









